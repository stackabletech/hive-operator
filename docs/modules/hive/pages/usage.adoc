= Usage

== Requirements
Apache Hive Metastores need a relational database to store their state.
We currently support https://www.postgresql.org/[PostgreSQL] and https://db.apache.org/derby/[Apache Derby] (embedded database, not recommended for production).
Other databases might work if JDBC drivers are available.
Please open an https://github.com/stackabletech/hive-operator/issues[issue] if you require support for another database.

== S3 Support

Hive supports creating tables in S3 compatible object stores.
To use this feature you need to provide connection details for the object store using the xref:concepts:s3.adoc[S3Connection] in the top level `clusterConfig`.

An example usage can look like this:

[source,yaml]
----
clusterConfig:
  s3:
    inline:
      host: minio
      port: 9000
      accessStyle: Path
      credentials:
        secretClass: simple-hive-s3-secret-class
----

== Apache HDFS Support

As well as S3, Hive also supports creating tables in HDFS.
You can add the HDFS connection in the top level `clusterConfig` as follows:

[source,yaml]
----
clusterConfig:
  hdfs:
    configMap: my-hdfs-cluster # Name of the HdfsCluster
----

== Monitoring

The managed Hive instances are automatically configured to export Prometheus metrics. See
xref:operators:monitoring.adoc[] for more details.

== Log aggregation

The logs can be forwarded to a Vector log aggregator by providing a discovery
ConfigMap for the aggregator and by enabling the log agent:

[source,yaml]
----
spec:
  clusterConfig:
    vectorAggregatorConfigMapName: vector-aggregator-discovery
  metastore:
    config:
      logging:
        enableVectorAgent: true
----

Further information on how to configure logging, can be found in
xref:concepts:logging.adoc[].

== Configuration & Environment Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Overriding certain properties, which are set by the operator (such as the HTTP port) can interfere with the operator and can lead to problems.

=== Configuration Properties

For a role or role group, at the same level of `config`, you can specify: `configOverrides` for the `hive-site.xml`. For example, if you want to set the `datanucleus.connectionPool.maxPoolSize` for the metastore to 20 adapt the `metastore` section of the cluster resource like so:

[source,yaml]
----
metastore:
  roleGroups:
    default:
      config: [...]
      configOverrides:
        hive-site.xml:
          datanucleus.connectionPool.maxPoolSize: "20"
      replicas: 1
----

Just as for the `config`, it is possible to specify this at role level as well:

[source,yaml]
----
metastore:
  configOverrides:
    hive-site.xml:
      datanucleus.connectionPool.maxPoolSize: "20"
  roleGroups:
    default:
      config: [...]
      replicas: 1
----

All override property values must be strings. The properties will be formatted and escaped correctly into the XML file.

For a full list of configuration options we refer to the Hive https://cwiki.apache.org/confluence/display/hive/configuration+properties[Configuration Reference].

=== Environment Variables

In a similar fashion, environment variables can be (over)written. For example per role group:

[source,yaml]
----
metastore:
  roleGroups:
    default:
      config: {}
      envOverrides:
        MY_ENV_VAR: "MY_VALUE"
      replicas: 1
----

or per role:

[source,yaml]
----
metastore:
  envOverrides:
    MY_ENV_VAR: "MY_VALUE"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

=== Resource Requests

include::home:concepts:stackable_resource_requests.adoc[]

A minimal HA setup consisting of 2 Hive metastore instances has the following https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[resource requirements]:

* `100m` CPU request
* `3000m` CPU limit
* `1280m` memory request and limit

Of course, additional services, require additional resources. For Stackable components, see the corresponding documentation on further resource requirements.

Corresponding to the values above, the operator uses the following resource defaults:

[source,yaml]
----
metastore:
  roleGroups:
    default:
      config:
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "512Mi"
----

The operator may configure an additional container for log aggregation. This is done when log aggregation is configured as described in xref:concepts:logging.adoc[]. The resources for this container cannot be configured using the mechanism described above. Use xref:home:concepts:overrides.adoc#_pod_overrides[podOverrides] for this purpose.

You can configure your own resource requests and limits by following the example above.

For more details regarding Kubernetes CPU limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[Assign CPU Resources to Containers and Pods].

== Examples

Please note that the version you need to specify is not only the version of Apache Hive which you want to roll out, but has to be amended with a Stackable version as shown.
This Stackable version is the version of the underlying container image which is used to execute the processes.
For a list of available versions please check our https://repo.stackable.tech/#browse/browse:docker:v2%2Fstackable%2Fhive%2Ftags[image registry].
It should generally be safe to simply use the latest image version that is available.

.Create a single node Apache Hive Metastore cluster using Derby:
[source,yaml]
----
---
apiVersion: hive.stackable.tech/v1alpha1
kind: HiveCluster
metadata:
  name: simple-hive-derby
spec:
  image:
    productVersion: 3.1.3
    stackableVersion: 0.prerelease
  clusterConfig:
    database:
      connString: jdbc:derby:;databaseName=/tmp/metastore_db;create=true
      user: APP
      password: mine
      dbType: derby
  metastore:
    roleGroups:
      default:
        replicas: 1
----

WARNING: You should not use the `Derby` database with more than one replica or in production. Derby stores data locally and therefore the data is not shared between different metastore Pods and lost after Pod restarts.

To create a single node Apache Hive Metastore (v2.3.9) cluster with derby and S3 access, deploy a minio (or use any available S3 bucket):
[source,bash]
----
helm install minio \
    minio \
    --repo https://charts.bitnami.com/bitnami \
    --set auth.rootUser=minio-access-key \
    --set auth.rootPassword=minio-secret-key
----

In order to upload data to minio we need a port-forward to access the web ui.
[source,bash]
----
kubectl port-forward service/minio 9001
----
Then, connect to localhost:9001 and login with the user `minio-access-key` and password `minio-secret-key`. Create a bucket and upload data.

Deploy the hive cluster:
[source,yaml]
----
---
apiVersion: hive.stackable.tech/v1alpha1
kind: HiveCluster
metadata:
  name: simple-hive-derby
spec:
  image:
    productVersion: 3.1.3
    stackableVersion: 0.prerelease
  clusterConfig:
    database:
      connString: jdbc:derby:;databaseName=/stackable/metastore_db;create=true
      user: APP
      password: mine
      dbType: derby
    s3:
      inline:
        host: minio
        port: 9000
        accessStyle: Path
        credentials:
          secretClass: simple-hive-s3-secret-class
  metastore:
    roleGroups:
      default:
        replicas: 1
---
apiVersion: secrets.stackable.tech/v1alpha1
kind: SecretClass
metadata:
  name: simple-hive-s3-secret-class
spec:
  backend:
    k8sSearch:
      searchNamespace:
        pod: {}
---
apiVersion: v1
kind: Secret
metadata:
  name: simple-hive-s3-secret
  labels:
    secrets.stackable.tech/class: simple-hive-s3-secret-class
stringData:
  accessKey: minio-access-key
  secretKey: minio-secret-key
----


To create a single node Apache Hive Metastore using PostgreSQL, deploy a PostgreSQL instance via helm.

[sidebar]
PostgreSQL introduced a new way to encrypt its passwords in version 10.
This is called `scram-sha-256` and has been the default as of PostgreSQL 14.
Unfortunately, Hive up until the latest 3.3.x version ships with JDBC drivers that do https://wiki.postgresql.org/wiki/List_of_drivers[_not_ support] this method.
You might see an error message like this:
`The authentication type 10 is not supported.`
If this is the case please either use an older PostgreSQL version or change its https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-PASSWORD-ENCRYPTION[`password_encryption`] setting to `md5`.

This installs PostgreSQL in version 10 to work around the issue mentioned above:
[source,bash]
----
helm install hive bitnami/postgresql --version=12.1.5 \
--set postgresqlUsername=hive \
--set postgresqlPassword=hive \
--set postgresqlDatabase=hive
----

.Create Hive Metastore using a PostgreSQL database
[source,yaml]
----
apiVersion: hive.stackable.tech/v1alpha1
kind: HiveCluster
metadata:
  name: simple-hive-postgres
spec:
  image:
    productVersion: 3.1.3
    stackableVersion: 0.prerelease
  clusterConfig:
    database:
      connString: jdbc:postgresql://hive-postgresql.default.svc.cluster.local:5432/hive
      user: hive
      password: hive
      dbType: postgres
  metastore:
    roleGroups:
      default:
        replicas: 1
----

=== Pod Placement

You can configure Pod placement for Hive metastores as described in xref:concepts:pod_placement.adoc[].

By default, the operator configures the following Pod placement constraints:

[source,yaml]
----
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - podAffinityTerm:
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: hive
            app.kubernetes.io/instance: cluster-name
            app.kubernetes.io/component: metastore
        topologyKey: kubernetes.io/hostname
      weight: 70
----

In the example above `cluster-name` is the name of the HiveCluster custom resource that owns this Pod.
